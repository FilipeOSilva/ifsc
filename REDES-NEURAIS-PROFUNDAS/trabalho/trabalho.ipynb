{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# PFL0090 - TRABALHO DE REDES NEURAIS ARTIFICIAIS PROFUNDAS\n",
    "\n",
    "Alunos: \n",
    "* ARTUR ANTONIO DAL PRÁ\n",
    "* FILIPE DA SILVA\n",
    "\n",
    "## Indice\n",
    "\n",
    "1. [Objetivo](#1)\n",
    "2. [Abordagens comuns e trabalhos correlatos](#2)\n",
    "3. [Proposta implementada](#3)\n",
    "4. [Resultados alcançados](#4)\n",
    "5. [Discussão final](#5)\n",
    "6. [Referências](#6)\n",
    "\n",
    "## Objetivo <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "Temos como meta, conseguir segmementar terrenos, visando ter noção de propriedades rurais podendo ter noção de valor, atraves de images de satalete, em 5 grupos, que são:\n",
    "* Construções; \n",
    "* Campos; \n",
    "* Florestas;\n",
    "* Estradas;\n",
    "* Extenções de Água;\n",
    "\n",
    "## Abordagens comuns e trabalhos correlatos <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "Para esse tipo de trabalho, quando executado de forma manual, é necessário ter uma pessoa que analisava as imagens e as marcava manualmente, realizando medições e segmentando os terrenos.\n",
    "\n",
    "Utilizaríamos o **método de Otsu**, que é um dos algoritmos de limiarização mais populares, utilizado para encontrar um threshold ideal para separar os elementos na frente e no fundo de uma imagem.\n",
    "Adaptando essa técnica à nossa realidade, imaginamos uma solução para esse problema utilizando técnicas computacionais. Primeiro, aplicaríamos um filtro de cores para evidenciar o limiar e, em seguida, um filtro de borda para obter uma transição nítida entre os elementos desejados. \n",
    "\n",
    "![Exemplo de campo por diferenca de cores](img/img_d_02.png)\n",
    "\n",
    "Após essas etapas, comprimiríamos as imagens e as aplicaríamos em uma rede neural clássica, considerando que cada pixel seria uma entrada da rede.\n",
    "\n",
    "## Proposta implementada e Resultados alcançados<a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "Das tecnicas estudadas, optamos pela [YOLO8](https://yolov8.com/) que em seu site é definica como \"Um novo estado da arte em visão computacional, suportando tarefas de detecção, classificação e segmentação de objetos\". Tendo em mente que o mesmo possui uma API de facil configuração e ser open source sendo isso fatores decisivos para a escolha.\n",
    "Para os dados, usamos uma base disponivel no site do [Roboflow](https://universe.roboflow.com/myproject-gccot/landscape-segmentation/dataset/5/download). Esse dataset possui 2696 imagens para treinamento e 289 para validação.\n",
    "\n",
    "![logo yolo8](img/img_d_03.png)  ![logo roboflow](img/img_d_04.png) \n",
    "\n",
    "Para isso decidimos fazer 3 treinamentos, que serão apresentados a seguir.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO, settings\n",
    "\n",
    "settings.update({'runs_dir': './runs'})\n",
    "\n",
    "model = YOLO('yolov8n-seg')  # yolov8n_custom.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento 1\n",
    "\n",
    "Foi criada uma rede com o treinamento para 30 epocas, sem nenhuma opçao de otmizacao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(data='data.yaml',\n",
    "            epochs=30,\n",
    "            imgsz=640\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Saida do treinamento da primeira rede.](img/img_a_01.png)\n",
    "\n",
    "Como podemos ver a saida do treinamento da primeira rede, levou mais de 5 horas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Treinamento gerou o seguintes graficos:\n",
    "\n",
    "![Saida do treinamento da primeira rede.](img/img_a_02.png)\n",
    "\n",
    "Onde podemos observar as epocas por treinamento.\n",
    "\n",
    "![Saida do treinamento da primeira rede.](img/img_a_03.png)\n",
    "\n",
    "Aqui temos a matriz de confusão normalizada.\n",
    "\n",
    "E por fim alguns exemplos da segmentação:\n",
    "\n",
    "![Saida do treinamento da primeira rede.](img/img_a_04.jpg)\n",
    "\n",
    "![Saida do treinamento da primeira rede.](img/img_a_05.jpg)\n",
    "\n",
    "![Saida do treinamento da primeira rede.](img/img_a_06.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento 2\n",
    "\n",
    "Para o caso 2 foi criada uma rede com o treinamento para 100 epocas, sem nenhuma opçao de otmizacao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train(data='data.yaml',\n",
    "            epochs=100,\n",
    "            imgsz=640\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Saida do treinamento da segunda rede.](img/img_b_01.png)\n",
    "\n",
    "Como podemos ver a saida do treinamento da segunda rede, levou mais de 17 horas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Treinamento gerou o seguintes graficos:\n",
    "\n",
    "![Saida do treinamento da segunda rede.](img/img_b_02.png)\n",
    "\n",
    "Onde podemos observar as epocas por treinamento.\n",
    "\n",
    "![Saida do treinamento da segunda rede.](img/img_b_03.png)\n",
    "\n",
    "Aqui temos a matriz de confusão normalizada.\n",
    "\n",
    "E por fim alguns exemplos da segmentação:\n",
    "\n",
    "![Saida do treinamento da segunda rede.](img/img_b_04.jpg)\n",
    "\n",
    "![Saida do treinamento da segunda rede.](img/img_b_05.jpg)\n",
    "\n",
    "![Saida do treinamento da segunda rede.](img/img_b_06.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento 3\n",
    "\n",
    "E para o ultimo caso utilizou-se uma rede com 30 epocas, com fatores de otimizacao, sendo o de \"patience\" o que indica o fim quando existe pouco avanço dentro do treinamento de epocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train(data='data.yaml',\n",
    "            epochs=30,\n",
    "            patience=8,\n",
    "            batch=16,  # number of images per batch (-1 for AutoBatch)\n",
    "            imgsz=640,\n",
    "            workers=8,\n",
    "            pretrained=True,\n",
    "            resume=False,  # resume training from last checkpoint\n",
    "            single_cls=False,  # Whether all classes will be the same (just one class)\n",
    "            # project='runs/detect',  # Default = /home/{user}/Documents/ultralytics/runs\n",
    "            box=7.5,  # More recall, better IoU, less precission, \n",
    "            cls=0.5,  # Bbox class better\n",
    "            dfl=1.5,  # Distribution Focal Loss. Better bbox boundaries\n",
    "            val=True,\n",
    "            # Augmentations\n",
    "            degrees=0.3,\n",
    "            hsv_s=0.3,\n",
    "            hsv_v=0.3,\n",
    "            scale=0.5,\n",
    "            fliplr=0.5\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Saida do treinamento da terceira rede.](img/img_c_01.png)\n",
    "\n",
    "Como podemos ver a saida do treinamento da terceira rede, levou mais de 1 hora e 30 minutos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Treinamento gerou o seguintes graficos:\n",
    "\n",
    "![Saida do treinamento da terceira rede.](img/img_c_02.png)\n",
    "\n",
    "Onde podemos observar as epocas por treinamento.\n",
    "\n",
    "![Saida do treinamento da terceira rede.](img/img_c_03.png)\n",
    "\n",
    "Aqui temos a matriz de confusão normalizada.\n",
    "\n",
    "E por fim alguns exemplos da segmentação:\n",
    "\n",
    "![Saida do treinamento da terceira rede.](img/img_c_04.jpg)\n",
    "\n",
    "![Saida do treinamento da terceira rede.](img/img_c_05.jpg)\n",
    "\n",
    "![Saida do treinamento da terceira rede.](img/img_c_06.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussão final <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "Observamos que todos os modelos alcançaram a convergência e estão prontos para serem implementados em produção como uma primeira versão. Para aumentar a precisão dos modelos, poderíamos considerar o uso de uma base de imagens de satélite. Como estamos utilizando o Google Maps nas atividades práticas da disciplina, essa abordagem poderia proporcionar uma acurácia superior. Isso se deve ao fato de que o conjunto de treinamento é definido pela altura de voo do drone, conforme especificado na descrição do dataset, que varia de acordo com o zoom e o controle de altitude.\n",
    "\n",
    "![Imagem obtida pelo modelo 2 com google maps.](img/img_d_01.png)\n",
    "\n",
    "Ao analisar a matriz de confusão, notamos que a segmentação referente à área de floresta frequentemente se confunde com o background. Isso é atribuído ao formato das florestas segmentadas no dataset estrangeiro. Na nossa região, a vegetação nativa apresenta afloramentos de rochas e não temos uma continuidade tendo esparçamento em alguns trechos (algumas vezes devido ao desmatamento irregular ou outras questões geologicas), resultando em uma cobertura florestal irregular.\n",
    "\n",
    "![Matriz de confusao.](img/img_b_03.png)\n",
    "\n",
    "Para melhorar as previsões no futuro, aprimorar a qualidade das imagens obtidas via satélite, considerando também uma refatoração do dataset, seria benéfico. Isso proporcionaria imagens mais nítidas e detalhadas, beneficiando a identificação de elementos como construções. Atualmente, a falta de nitidez nessas imagens afeta a resolução das previsões, e uma melhoria nesse aspecto resultaria em uma predição mais precisa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo prático\n",
    "\n",
    "Iremos agora usar um metodo de captura de tela para por a prova os modelos obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageGrab\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def real_time_maps(model):\n",
    "\n",
    "    track_history = defaultdict(lambda: [])\n",
    "\n",
    "    while True:\n",
    "        img = ImageGrab.grab(bbox=(100,100,1000,1000))\n",
    "    \n",
    "        results = model.track(img, persist=True)\n",
    "    \n",
    "        # Process results list\n",
    "        for result in results:\n",
    "            # Visualize the results on the frame\n",
    "            img = result.plot()\n",
    "    \n",
    "            if True:\n",
    "                try:\n",
    "                    # Get the boxes and track IDs\n",
    "                    boxes = result.boxes.xywh.cpu()\n",
    "                    track_ids = result.boxes.id.int().cpu().tolist()\n",
    "    \n",
    "                    # Plot the tracks\n",
    "                    for box, track_id in zip(boxes, track_ids):\n",
    "                        x, y, w, h = box\n",
    "                        track = track_history[track_id]\n",
    "                        track.append((float(x), float(y)))  # x, y center point\n",
    "                        if len(track) > 30:  # retain 90 tracks for 90 frames\n",
    "                            track.pop(0)\n",
    "    \n",
    "                        # Draw the tracking lines\n",
    "                        points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "                        cv2.polylines(img, [points], isClosed=False, color=(230, 0, 0), thickness=5)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "        cv2.imshow('Imagem do GoogleMaps', img)\n",
    "    \n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord('q'):\n",
    "            break\n",
    "        \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/segment/train5/weights/best.pt')\n",
    "\n",
    "real_time_maps(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/segment/train52/weights/best.pt')\n",
    "\n",
    "real_time_maps(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/segment/train522/weights/best.pt')\n",
    "\n",
    "real_time_maps(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências  <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "* https://universe.roboflow.com/myproject-gccot/landscape-segmentation/dataset/5/download\n",
    "* https://github.com/WellingtonDev25/yolov8_amostra/blob/main/YoloV8_Amostra.ipynb\n",
    "* https://csvisaocomputacional.com.br/cursoyolo\n",
    "* https://docs.ultralytics.com/modes/track/#multithreaded-tracking\n",
    "* https://www.kaggle.com/datasets/sshikamaru/car-object-detection/code\n",
    "* http://profs.ic.uff.br/~aconci/OtsuTexto.pdf\n",
    "* https://www.youtube.com/watch?v=JVqU9IBZaAg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
